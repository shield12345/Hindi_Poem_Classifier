{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZmnOjT7_HJb",
        "outputId": "4dee7bfb-5ef3-421d-fa47-f31d36747124"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5n0p7qA3Q7DM",
        "outputId": "ba28c15c-abcb-4cae-c617-98064ebafc41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start btf\n",
            "35566\n",
            "3952\n",
            "end btf\n"
          ]
        }
      ],
      "source": [
        "# Import libraries.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "processed_data_path = './processed_data.txt'\n",
        "\n",
        "# The file is written to be run on Colab with the drive mounted and the folders created along with the inputs stored\n",
        "# with open(\"/content/drive/My Drive/ISI-Project/processed_data.txt\",\"rb\") as f:\n",
        "# \tall_data = pickle.load(f)\n",
        " \n",
        "with open (processed_data_path,'rb') as fin:\n",
        "  all_data = pickle.load(fin)\n",
        "\n",
        "# Randomly shuffle the data\n",
        "# all_data = all_data[0:100]\n",
        "random.shuffle(all_data)\n",
        "\n",
        "# Find the total length of the data\n",
        "TotalLen = len(all_data)\n",
        "# 90% of the total length is the Train set length\n",
        "TrainLen = int(90*TotalLen/100)\n",
        "# Remaining is the test set\n",
        "TestLen = TotalLen - TrainLen\n",
        "\n",
        "# Get the train and test set via splicing\n",
        "TrainSet = all_data[0:TrainLen]\n",
        "TestSet = all_data[TrainLen:]\n",
        "\n",
        "# Train_poems is the set of all the poems only which will be used to train the model\n",
        "train_poems = []\n",
        "\n",
        "# Store the poems from the train set into it\n",
        "for elem in TrainSet:\n",
        "\ttrain_poems.append(elem[4])\n",
        "\n",
        "def identity_func(doc):\n",
        "\treturn doc\n",
        "\n",
        "# Start the working on btf\n",
        "print(\"Start btf\")\n",
        "# Intialize the model\n",
        "btf = TfidfVectorizer(binary=True,\n",
        "\tnorm=None,\n",
        "\tuse_idf=False,\n",
        "\tsmooth_idf=False,\n",
        "\tlowercase=False,\n",
        "\ttokenizer=identity_func,\n",
        "\tpreprocessor=identity_func,\n",
        "    min_df = 500,\n",
        "\tmax_df = 0.8,\n",
        "\ttoken_pattern=None)\n",
        "# Fit the model\n",
        "btf.fit(train_poems)\n",
        "# Find the vector for the poems in the train set and store them too.\n",
        "TrainSetWithVector_btf = []\n",
        "for elem in TrainSet:\n",
        "\telem.append(btf.transform([elem[4]]).toarray()[0].tolist())\n",
        "\tTrainSetWithVector_btf.append(elem)\n",
        "\n",
        "# Find the vector for the poems in the test set and store them too.\n",
        "TestSetWithVector_btf = []\n",
        "for elem in TestSet:\n",
        "\telem.append(btf.transform([elem[4]]).toarray()[0].tolist())\n",
        "\tTestSetWithVector_btf.append(elem)\n",
        "\n",
        "print(len(TrainSetWithVector_btf))\n",
        "print(len(TestSetWithVector_btf))\n",
        "\n",
        "# Store the output vectors as pickle dumps\n",
        "with open(\"/content/drive/My Drive/NLP_Project/train_data_btf.txt\",\"wb\") as f:\n",
        "\tpickle.dump(TrainSetWithVector_btf,f)\n",
        " \n",
        "with open('/content/drive/My Drive/NLP_Project/test_data_btf.txt', 'wb') as f:\n",
        "   pickle.dump(TestSetWithVector_btf, f)\n",
        "\n",
        "print(\"end btf\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start bow\")\n",
        "# Intialize the model\n",
        "bow = TfidfVectorizer(binary=False,\n",
        "\tnorm=None,\n",
        "\tuse_idf=False,\n",
        "\tsmooth_idf=False,\n",
        "\tlowercase=False,\n",
        "\ttokenizer=identity_func,\n",
        "\tpreprocessor=identity_func,\n",
        "  min_df = 500,\n",
        "\tmax_df = 0.8,\n",
        "\ttoken_pattern=None)\n",
        "# Fit the model\n",
        "bow.fit(train_poems)\n",
        "# Find the vector for the poems in the train set and store them too.\n",
        "TrainSetWithVector_bow = []\n",
        "for elem in TrainSet:\n",
        "\telem[5] = (bow.transform([elem[4]]).toarray()[0].tolist())\n",
        "\tTrainSetWithVector_bow.append(elem)\n",
        "\n",
        "# Find the vector for the poems in the test set and store them too.\n",
        "TestSetWithVector_bow = []\n",
        "for elem in TestSet:\n",
        "\telem[5] = (bow.transform([elem[4]]).toarray()[0].tolist())\n",
        "\tTestSetWithVector_bow.append(elem)\n",
        "\n",
        "print(len(TrainSetWithVector_bow))\n",
        "print(len(TestSetWithVector_bow))\n",
        "\n",
        "# Store the output vectors as pickle dumps\n",
        "with open(\"/content/drive/My Drive/NLP_Project/train_data_bow.txt\",\"wb\") as f:\n",
        "\tpickle.dump(TrainSetWithVector_bow,f)\n",
        " \n",
        "with open(\"/content/drive/My Drive/NLP_Project/test_data_bow.txt\",\"wb\") as f:\n",
        "\tpickle.dump(TestSetWithVector_bow,f)\n",
        "print(\"end bow\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiOCdtPqxwWM",
        "outputId": "49e61f34-cf81-4bcf-bd2f-30edbcbf3434"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start bow\n",
            "35566\n",
            "3952\n",
            "end bow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start tfl1\")\n",
        "# Intialize the model\n",
        "tfl1 = TfidfVectorizer(binary=False,\n",
        "\tnorm='l1',\n",
        "\tuse_idf=False,\n",
        "\tsmooth_idf=False,\n",
        "\tlowercase=False,\n",
        "\ttokenizer=identity_func,\n",
        "\tpreprocessor=identity_func,\n",
        "  min_df = 500,\n",
        "\tmax_df = 0.8,\n",
        "\ttoken_pattern=None)\n",
        "\n",
        "# Fit the model\n",
        "tfl1.fit(train_poems)\n",
        "# Find the vector for the poems in the train set and store them too.\n",
        "TrainSetWithVector_tfl1 = []\n",
        "for elem in TrainSet:\n",
        "\telem[5] = (tfl1.transform([elem[4]]).toarray()[0].tolist())\n",
        "\tTrainSetWithVector_tfl1.append(elem)\n",
        "\n",
        "# Find the vector for the poems in the test set and store them too.\n",
        "TestSetWithVector_tfl1 = []\n",
        "for elem in TestSet:\n",
        "\telem[5] = (tfl1.transform([elem[4]]).toarray()[0].tolist())\n",
        "\tTestSetWithVector_tfl1.append(elem)\n",
        "\n",
        "print(len(TrainSetWithVector_tfl1))\n",
        "print(len(TestSetWithVector_tfl1))\n",
        "\n",
        "# Store the output vectors as pickle dumps\n",
        "with open(\"/content/drive/My Drive/NLP_Project/train_data_tfl1.txt\",\"wb\") as f:\n",
        "\tpickle.dump(TrainSetWithVector_tfl1,f)\n",
        " \n",
        "with open(\"/content/drive/My Drive/NLP_Project/test_data_tfl1.txt\",\"wb\") as f:\n",
        "\tpickle.dump(TestSetWithVector_tfl1,f)\n",
        "\n",
        "print(\"end tfl1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQnaXeu46xtw",
        "outputId": "de7d601a-35db-4b40-919e-dccc8e5167d0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start tfl1\n",
            "35566\n",
            "3952\n",
            "end tfl1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start tfidf\")\n",
        "# Intialize the model\n",
        "tfidf = TfidfVectorizer(binary=False,\n",
        "\tnorm='l2',\n",
        "\tuse_idf=True,\n",
        "\tsmooth_idf=True,\n",
        "\tlowercase=False,\n",
        "\ttokenizer=identity_func,\n",
        "\tpreprocessor=identity_func,\n",
        "  min_df = 500,\n",
        "\tmax_df = 0.8,\n",
        "\ttoken_pattern=None)\n",
        "\n",
        "# Fit the model\n",
        "tfidf.fit(train_poems)\n",
        "# Find the vector for the poems in the train set and store them too.\n",
        "TrainSetWithVector_tfidf = []\n",
        "for elem in TrainSet:\n",
        "\telem[5] = (tfidf.transform([elem[4]]).toarray()[0].tolist())\n",
        "\tTrainSetWithVector_tfidf.append(elem)\n",
        "\n",
        "# Find the vector for the poems in the test set and store them too.\n",
        "TestSetWithVector_tfidf = []\n",
        "for elem in TestSet:\n",
        "\telem[5] = (tfidf.transform([elem[4]]).toarray()[0].tolist())\n",
        "\tTestSetWithVector_tfidf.append(elem)\n",
        "\n",
        "print(len(TrainSetWithVector_tfidf))\n",
        "print(len(TestSetWithVector_tfidf))\n",
        "\n",
        "# Store the output vectors as pickle dumps\n",
        "with open(\"/content/drive/My Drive/NLP_Project/train_data_tfidf.txt\",\"wb\") as f:\n",
        "\tpickle.dump(TrainSetWithVector_tfidf,f)\n",
        " \n",
        "with open(\"/content/drive/My Drive/NLP_Project/test_data_tfidf.txt\",\"wb\") as f:\n",
        "\tpickle.dump(TestSetWithVector_tfidf,f)\n",
        "\t\n",
        "print(\"end tfidf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9cRbKOk60AZ",
        "outputId": "fb1986e1-71cc-43f3-d477-8143e4f69450"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start tfidf\n",
            "35566\n",
            "3952\n",
            "end tfidf\n"
          ]
        }
      ]
    }
  ]
}